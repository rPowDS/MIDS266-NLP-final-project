{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMgmBPX88/JMZF25tQXbhcz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d747d47cc0e546a380d19577f188a36a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ac049d9320ec4a13b0787ce67db65c94","IPY_MODEL_f0f9571c09804b5888c4ceee322e7893","IPY_MODEL_9e945e4716134105a98bf78f02294ab3"],"layout":"IPY_MODEL_92f6de43fb61486eb886b95a5c5b668e"}},"ac049d9320ec4a13b0787ce67db65c94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff9db24efbd84adda0514ff23243b9a9","placeholder":"​","style":"IPY_MODEL_4c25cf5915dc4288b46fde1d8f49ab26","value":"README.md: "}},"f0f9571c09804b5888c4ceee322e7893":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ddae9b8c1ae4013bdf6bd746f0b477d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_07ee09d43ad0417cba7ab2478905e90b","value":1}},"9e945e4716134105a98bf78f02294ab3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ff15435a5cf42c3bfc15f96bdad18e1","placeholder":"​","style":"IPY_MODEL_79738e0499944cebbac7719cc5b0f330","value":" 15.6k/? [00:00&lt;00:00, 1.77MB/s]"}},"92f6de43fb61486eb886b95a5c5b668e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff9db24efbd84adda0514ff23243b9a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c25cf5915dc4288b46fde1d8f49ab26":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ddae9b8c1ae4013bdf6bd746f0b477d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"07ee09d43ad0417cba7ab2478905e90b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1ff15435a5cf42c3bfc15f96bdad18e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79738e0499944cebbac7719cc5b0f330":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3cc9da5d05af4913b483120dbe185f87":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1c646d4b367a479e8fcdecf96da2efbd","IPY_MODEL_792abf632ee84d5f8a3785639ccf6481","IPY_MODEL_c1b1b1adccfb44a2a672ff9fd76c29fa"],"layout":"IPY_MODEL_4d44ff2a01fa45579bd9a1a81cad4685"}},"1c646d4b367a479e8fcdecf96da2efbd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da75304c769c4dd9a6e73fb6d85b7ad3","placeholder":"​","style":"IPY_MODEL_89c883301abe411592ff8523e6f7346d","value":"100%"}},"792abf632ee84d5f8a3785639ccf6481":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_34694496a05a42fb9116659727dff6f0","max":2000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_198102424eba47788eab431ca803bbe9","value":2000}},"c1b1b1adccfb44a2a672ff9fd76c29fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e6146a9b2044126b69ba5b212858fbc","placeholder":"​","style":"IPY_MODEL_2d191fb36851480eac35e3f523e8c696","value":" 2000/2000 [27:15&lt;00:00,  1.04it/s]"}},"4d44ff2a01fa45579bd9a1a81cad4685":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da75304c769c4dd9a6e73fb6d85b7ad3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89c883301abe411592ff8523e6f7346d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34694496a05a42fb9116659727dff6f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"198102424eba47788eab431ca803bbe9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8e6146a9b2044126b69ba5b212858fbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d191fb36851480eac35e3f523e8c696":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["## Candidate Generation Strategy\n","\n","### Why Generate Multiple Candidates?\n","\n","The core hypothesis of the project is that beam search produces multiple candidate summaries, and **not all candidates are equally factual**. By generating K candidates and selecting the most factual one, it may reduce hallucinations without retraining the model.\n","\n","### Why Beam Search?\n","\n","Using **beam search** rather than sampling for several reasons:\n","\n","1. **Deterministic**: The same input always produces the same candidates, ensuring reproducibility.\n","2. **High quality**: Beam search explores high-probability paths, producing fluent outputs.\n","3. **Diverse but related**: Candidates share common structure but differ in specific details.\n","\n","### Generation Parameters\n","\n","| Parameter | Value | Rationale |\n","|-----------|-------|-----------|\n","| `num_beams` | 5 | Number of parallel hypotheses to track |\n","| `num_return_sequences` | 5 | Return all 5 beam hypotheses as candidates |\n","| `max_new_tokens` | 128 | Sufficient for CNN/DailyMail summaries (~56 tokens avg) |\n","| `min_new_tokens` | 30 | Ensures summaries are not trivially short |\n","| `do_sample` | False | Deterministic beam search, not stochastic sampling |\n","\n","### Why K=5?\n","\n","- **K=1**: No reranking possible (just the baseline)\n","- **K=2-3**: Limited diversity; may not include a factual candidate\n","- **K=5**: Good trade-off between diversity and compute cost\n","- **K>5**: Diminishing returns (see K-ablation in Notebook 05)"],"metadata":{"id":"cAzbix_ArRyM"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9NmYVJXjzki7","executionInfo":{"status":"ok","timestamp":1763316124793,"user_tz":480,"elapsed":40000,"user":{"displayName":"R Powers","userId":"06683789047320527470"}},"outputId":"40d29e96-d284-47cb-f323-5fd6151a42e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- 1.0: Setup ---\n","Mounted at /content/drive\n","Loaded config from: /content/drive/MyDrive/w266_project_final/configs/baseline.json\n","Using device: cuda\n","GPU Name: NVIDIA A100-SXM4-80GB\n","Will generate K=5 candidates per article.\n"]}],"source":["\n","\n","import os\n","import json\n","import torch\n","import pandas as pd\n","import orjson\n","from google.colab import drive\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","from tqdm.auto import tqdm # For a nice progress bar\n","\n","print(\"--- 1.0: Setup ---\")\n","# 1.1: Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Central Config\n","PROJECT_ROOT = \"/content/drive/MyDrive/w266_project_final\"\n","CONFIGS_DIR = os.path.join(PROJECT_ROOT, \"configs\")\n","CONFIG_PATH = os.path.join(CONFIGS_DIR, \"baseline.json\")\n","\n","with open(CONFIG_PATH, 'r') as f:\n","    cfg = json.load(f)\n","\n","print(f\"Loaded config from: {CONFIG_PATH}\")\n","\n","#  Artifact Paths\n","DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n","OUTPUTS_DIR = os.path.join(PROJECT_ROOT, \"outputs\") # Use \"outputs\" dir\n","os.makedirs(OUTPUTS_DIR, exist_ok=True)\n","\n","# This is the fine-tuned model built in Notebook 02\n","CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, cfg['train']['output_dir'])\n","\n","#  NEW artifact we will create\n","CANDIDATES_FILE = os.path.join(OUTPUTS_DIR, \"validation_candidates_k5.jsonl\")\n","\n","# Setup\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")\n","if device == \"cuda\":\n","    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n","\n","# Define Generation Parameters from Config\n","gen_params = cfg['generate']\n","K_CANDIDATES = gen_params['num_return_sequences']\n","print(f\"Will generate K={K_CANDIDATES} candidates per article.\")"]},{"cell_type":"code","source":["\n","# Loading Model and Tokenizer\n","\n","print(\"\\n--- 2.0: Loading Fine-Tuned Model & Tokenizer ---\")\n","print(f\"Loading model from: {CHECKPOINT_DIR}\")\n","\n","# Load the fine-tuned model and tokenizer from checkpoint\n","tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_DIR)\n","model = AutoModelForSeq2SeqLM.from_pretrained(CHECKPOINT_DIR).to(device)\n","model.eval() # Setting model to evaluation mode\n","\n","print(\"Model and tokenizer loaded successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xkx75zda0CbQ","executionInfo":{"status":"ok","timestamp":1763316152427,"user_tz":480,"elapsed":23556,"user":{"displayName":"R Powers","userId":"06683789047320527470"}},"outputId":"73678b1f-8c62-4e25-86f6-24cfcc04aba8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- 2.0: Loading Fine-Tuned Model & Tokenizer ---\n","Loading model from: /content/drive/MyDrive/w266_project_final/models/bart_base_cnn_dm_20k\n","Model and tokenizer loaded successfully.\n"]}]},{"cell_type":"code","source":["\n","# Load and Prepare Dataset\n","\n","print(\"\\n--- 3.0: Loading Validation Dataset ---\")\n","# Loading the raw validation set (not the tokenized one)\n","# Use the 'val_subset_size' from the config to match notebook 02\n","val_subset_size = cfg['val_subset_size']\n","\n","raw_dataset = load_dataset(\n","    cfg[\"dataset_name\"],\n","    cfg[\"dataset_config\"],\n","    split=\"validation\",\n","    cache_dir=os.path.join(DATA_DIR, \"hf_cache\")\n",")\n","\n","# Applying the same shuffle and subset logic from training\n","val_ds = raw_dataset.shuffle(seed=cfg['seed']).select(range(val_subset_size))\n","\n","print(f\"Loaded and selected {len(val_ds)} examples from the validation set.\")\n","\n","# Get column names from config\n","SOURCE_COL = cfg['text_fields']['source']\n","SUMMARY_COL = cfg['text_fields']['summary']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":217,"referenced_widgets":["d747d47cc0e546a380d19577f188a36a","ac049d9320ec4a13b0787ce67db65c94","f0f9571c09804b5888c4ceee322e7893","9e945e4716134105a98bf78f02294ab3","92f6de43fb61486eb886b95a5c5b668e","ff9db24efbd84adda0514ff23243b9a9","4c25cf5915dc4288b46fde1d8f49ab26","7ddae9b8c1ae4013bdf6bd746f0b477d","07ee09d43ad0417cba7ab2478905e90b","1ff15435a5cf42c3bfc15f96bdad18e1","79738e0499944cebbac7719cc5b0f330"]},"id":"EGf4RzX40CZL","executionInfo":{"status":"ok","timestamp":1763316168613,"user_tz":480,"elapsed":12938,"user":{"displayName":"R Powers","userId":"06683789047320527470"}},"outputId":"d644dde4-b34c-435a-ac02-379380aa750a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- 3.0: Loading Validation Dataset ---\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["README.md: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d747d47cc0e546a380d19577f188a36a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loaded and selected 2000 examples from the validation set.\n"]}]},{"cell_type":"code","source":["\n","# Generate All Candidates (The Main Event)\n","print(f\"\\n--- 4.0: Generating {K_CANDIDATES} Candidates for {len(val_ds)} Articles ---\")\n","print(f\"This is the main GPU task. Saving results to: {CANDIDATES_FILE}\")\n","\n","# Writing the file line-by-line to save memory and prevent data loss\n","with open(CANDIDATES_FILE, 'wb') as f_out:\n","    for example in tqdm(val_ds):\n","        article_text = example[SOURCE_COL]\n","        reference_summary = example[SUMMARY_COL]\n","\n","        # Tokenize the article\n","        inputs = tokenizer(\n","            article_text,\n","            max_length=cfg['tokenization']['max_source_len'],\n","            truncation=True,\n","            return_tensors=\"pt\"\n","        ).to(device)\n","\n","        # Run model.generate()\n","        # This will use the parameters from config file\n","        with torch.no_grad():\n","            output_ids_batch = model.generate(\n","                inputs['input_ids'],\n","                num_beams=gen_params['num_beams'],\n","                num_return_sequences=gen_params['num_return_sequences'],\n","                max_new_tokens=gen_params['max_new_tokens'],\n","                min_new_tokens=gen_params['min_new_tokens'],\n","                do_sample=False # Use beam search, not sampling\n","            )\n","\n","        # Decode the results\n","        generated_summaries = tokenizer.batch_decode(\n","            output_ids_batch,\n","            skip_special_tokens=True\n","        )\n","\n","        # Create the data record\n","        record = {\n","            \"id\": example.get('id', None),\n","            \"article\": article_text,\n","            \"reference_summary\": reference_summary,\n","            f\"generated_candidates_k{K_CANDIDATES}\": generated_summaries\n","        }\n","\n","        f_out.write(orjson.dumps(record) + b'\\n')\n","\n","print(\"\\n--- 5.0: Candidate Generation Complete! ---\")\n","print(f\"All candidates have been saved to {CANDIDATES_FILE}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201,"referenced_widgets":["3cc9da5d05af4913b483120dbe185f87","1c646d4b367a479e8fcdecf96da2efbd","792abf632ee84d5f8a3785639ccf6481","c1b1b1adccfb44a2a672ff9fd76c29fa","4d44ff2a01fa45579bd9a1a81cad4685","da75304c769c4dd9a6e73fb6d85b7ad3","89c883301abe411592ff8523e6f7346d","34694496a05a42fb9116659727dff6f0","198102424eba47788eab431ca803bbe9","8e6146a9b2044126b69ba5b212858fbc","2d191fb36851480eac35e3f523e8c696"]},"id":"dq_4z07j0CWC","executionInfo":{"status":"ok","timestamp":1763317814512,"user_tz":480,"elapsed":1635954,"user":{"displayName":"R Powers","userId":"06683789047320527470"}},"outputId":"92e51460-9d55-4a9b-e701-770ff5166a46"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- 4.0: Generating 5 Candidates for 2000 Articles ---\n","This is the main GPU task. Saving results to: /content/drive/MyDrive/w266_project_final/outputs/validation_candidates_k5.jsonl\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2000 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cc9da5d05af4913b483120dbe185f87"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","--- 5.0: Candidate Generation Complete! ---\n","All candidates have been saved to /content/drive/MyDrive/w266_project_final/outputs/validation_candidates_k5.jsonl\n","You can now proceed to notebook 04 for reranking and analysis.\n","You will not need a high-power GPU for the next steps.\n"]}]},{"cell_type":"code","source":["import os\n","import orjson\n","from google.colab import drive\n","\n","# 1. Mounting drive\n","drive.mount('/content/drive')\n","\n","# 2. Define the path to new artifact\n","PROJECT_ROOT = \"/content/drive/MyDrive/w266_project_final\"\n","OUTPUTS_DIR = os.path.join(PROJECT_ROOT, \"outputs\")\n","CANDIDATES_FILE = os.path.join(OUTPUTS_DIR, \"validation_candidates_k5.jsonl\")\n","\n","# 3. Open the file\n","print(f\"Peeking inside: {CANDIDATES_FILE}\\n\")\n","\n","try:\n","    with open(CANDIDATES_FILE, 'rb') as f:\n","        first_line = f.readline()\n","\n","        if first_line:\n","\n","            data = orjson.loads(first_line)\n","\n","            # Print a summary\n","            print(\"--- Success! Found 1 record: ---\")\n","            print(f\"Article (truncated): {data['article'][:200]}...\")\n","            print(f\"\\nReference Summary: {data['reference_summary']}\")\n","\n","            print(f\"\\nGenerated Candidates (K={len(data['generated_candidates_k5'])}):\")\n","            for i, cand in enumerate(data['generated_candidates_k5']):\n","                print(f\"  {i+1}: {cand}\")\n","        else:\n","            print(\"File is empty!\")\n","\n","except FileNotFoundError:\n","    print(f\"ERROR: File not found at {CANDIDATES_FILE}\")\n","except Exception as e:\n","    print(f\"An error occurred: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wkCskhLO0CTE","executionInfo":{"status":"ok","timestamp":1763318126319,"user_tz":480,"elapsed":777,"user":{"displayName":"R Powers","userId":"06683789047320527470"}},"outputId":"3e94398a-f5d9-493a-d85b-e5f551f7b355"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Peeking inside: /content/drive/MyDrive/w266_project_final/outputs/validation_candidates_k5.jsonl\n","\n","--- Success! Found 1 record: ---\n","Article (truncated): Jarryd Hayne's move to the NFL is a boost for rugby league in the United States, it has been claimed. The Australia international full-back or centre quit the National Rugby League in October to try h...\n","\n","Reference Summary: Jarryd Hayne quit the NRL in October to try and get into American Football .\n","This week, he signed a three-year contract with the San Francisco 49ers .\n","The chairman of the US Association of Rugby League welcomed his arrival .\n","\n","Generated Candidates (K=5):\n","  1: Jarryd Hayne has signed a three-year contract with the San Francisco 49ers .\n","The Australia international quit the NRL in October to try his luck in American football .\n","Hayne could play at full back or centre in rugby league and is expected to be a running back .\n","  2: Jarryd Hayne has signed a three-year contract with the San Francisco 49ers .\n","The Australia international quit the NRL in October to try his luck in American football .\n","Hayne could play at full back or centre in rugby league .\n","  3: Jarryd Hayne has signed a three-year contract with the San Francisco 49ers .\n","The Australia international quit the NRL in October to try his luck in American football .\n","Hayne is expected to play at full back or centre in rugby league .\n","  4: Jarryd Hayne has signed a three-year contract with the San Francisco 49ers .\n","The Australia international quit the NRL in October to try his luck in American football .\n","  5: Jarryd Hayne has signed a three-year contract with the San Francisco 49ers .\n","The Australia international quit the NRL in October to try his luck in American football .\n","Hayne could play at full back or centre in rugby league and is expected to be a running back for the 49ers in 2017 .\n"]}]},{"cell_type":"markdown","source":["##Notebook 03 Summary & How to Interpret  Results\n","\n","What Did This Notebook Do?\n","\n","This notebook loaded the fine-tuned BART model (from Notebook 02) and looped through all 2,000 articles in the validation set. For each article, it ran model.generate() to create K=5 candidate summaries and saved them all to the validation_candidates_k5.jsonl file.\n","\n","##Breakingdown that output:\n","\n","**Article (truncated)**: This is the original input text from the CNN/DailyMail dataset that your model was asked to summarize.\n","\n","**Reference Summary**: This is the \"ground truth\" or \"gold standard\" summary written by a human. This is what we are comparing against for ROUGE scores.\n","\n","**Generated Candidates (K=5)**: These are the 5 different summaries your BART model produced.\n","\n","##How to Interpret. The \"Why\"\n","\n","This is the most important part. You might notice the 5 candidates look very similar to each other. This is normal and expected because of beam search.\n","\n","Beam search explores several \"paths\" to build a sentence. These 5 candidates are the 5 highest-scoring paths it found. They will often share the same beginning and differ by only a few words or a final sentence.\n","\n","This is the entire point of the project. The hypothesis is that even though these 5 summaries look similar, one of them is likely more factually accurate than the others. Right now, we just have 5 guesses. We have no \"judge.\""],"metadata":{"id":"J1U3TQ7Y-_6q"}}]}