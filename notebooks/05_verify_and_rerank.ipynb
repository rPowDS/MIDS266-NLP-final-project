{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a6f41e",
   "metadata": {},
   "source": [
    "# 05 — Verification (FactCC‑style) & Reranking\n",
    "\n",
    "**Goal:** Score each candidate summary for **factual consistency** vs. its source article, then **rerank** and compare against the baseline.\n",
    "We implement a **FactCC‑style NLI verifier** and a robust chunked scoring scheme (sentence‑wise max over source chunks → averaged).\n",
    "\n",
    "Format: **What/Why → Code (commented) → How to read results**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f6a221",
   "metadata": {},
   "source": [
    "## What / Why\n",
    "**What:** Install/load an **NLI verifier** checkpoint and basic utilities (sentence splitting + chunking).  \n",
    "**Why:** FactCC is an NLI‑style factuality classifier. If a dedicated FactCC checkpoint isn’t available, a strong **RoBERTa‑large NLI** model works well as a drop‑in verifier for entailment vs. contradiction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a6e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, subprocess, json, re, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs, check=True)\n",
    "\n",
    "pip_install([\n",
    "    \"transformers==4.41.2\",\n",
    "    \"datasets==2.19.1\",\n",
    "    \"evaluate==0.4.2\",\n",
    "    \"sentencepiece==0.1.99\",\n",
    "])\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "with open(\"configs/baseline.json\", \"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CANDIDATE_MODELS = [\n",
    "    \"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\",\n",
    "    \"roberta-large-mnli\",\n",
    "]\n",
    "loaded = False\n",
    "for m in CANDIDATE_MODELS:\n",
    "    try:\n",
    "        nli_tok = AutoTokenizer.from_pretrained(m, use_fast=True)\n",
    "        nli_model = AutoModelForSequenceClassification.from_pretrained(m).to(device)\n",
    "        nli_model.eval()\n",
    "        VERIFIER_NAME = m\n",
    "        loaded = True\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load\", m, \"->\", e)\n",
    "\n",
    "assert loaded, \"Could not load any NLI/FactCC checkpoint.\"\n",
    "print(\"Verifier:\", VERIFIER_NAME)\n",
    "\n",
    "SENT_RE = re.compile(r\"(?<=[.!?])\\s+\")\n",
    "def sent_split(text: str) -> List[str]:\n",
    "    sents = [s.strip() for s in SENT_RE.split(text) if s.strip()]\n",
    "    return sents if sents else [text.strip()]\n",
    "\n",
    "def chunk_source(text: str, max_tokens: int = 350, stride: int = 60) -> List[str]:\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk_words = words[i:i+max_tokens]\n",
    "        chunks.append(\" \".join(chunk_words))\n",
    "        i += max_tokens - stride if max_tokens > stride else max_tokens\n",
    "    return chunks if chunks else [text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2f4fb0",
   "metadata": {},
   "source": [
    "**How to read results:**  \n",
    "If you see `Verifier: <model-name>`, the NLI checkpoint loaded correctly. The regex sentence splitter and chunker keep dependencies light and fast.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1258d39",
   "metadata": {},
   "source": [
    "## What / Why\n",
    "**What:** Implement the **FactCC‑style score**: for each candidate sentence, compute the **max entailment probability** over all source chunks; the document‑level factuality is the **average** over sentences.  \n",
    "**Why:** This mirrors the intuition behind robust chunked verification used in factuality metrics and avoids context truncation issues on long articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b03dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def entailment_prob(premise: str, hypothesis: str) -> float:\n",
    "    inputs = nli_tok(premise, hypothesis, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    logits = nli_model(**inputs).logits\n",
    "    probs = torch.softmax(logits, dim=-1)[0].detach().cpu().numpy().tolist()\n",
    "    entail_idx = 2 if logits.shape[-1] >= 3 else int(np.argmax(probs))\n",
    "    return float(probs[entail_idx])\n",
    "\n",
    "def factcc_style_score(source: str, candidate: str, chunk_tokens: int = 350) -> float:\n",
    "    cand_sents = sent_split(candidate)\n",
    "    src_chunks = chunk_source(source, max_tokens=chunk_tokens, stride=60)\n",
    "    per_sent = []\n",
    "    for s in cand_sents:\n",
    "        if not s.strip():\n",
    "            continue\n",
    "        scores = [entailment_prob(ch, s) for ch in src_chunks]\n",
    "        per_sent.append(max(scores) if scores else 0.0)\n",
    "    return float(np.mean(per_sent)) if per_sent else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e38a0d",
   "metadata": {},
   "source": [
    "**How to read results:**  \n",
    "This function returns a **single scalar** in `[0, 1]` per candidate, higher is “more supported by the source.” We’ll apply it to each line of the candidates file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce12d7",
   "metadata": {},
   "source": [
    "## What / Why\n",
    "**What:** Load the **candidates JSONL** from Notebook 04, score each with the verifier, and save `verification_scores.jsonl`.  \n",
    "**Why:** We want a reusable artifact: `{id, candidate, factcc_score, rougeL (optional)}` that we can aggregate by article and compare strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4992bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "import evaluate as hf_evaluate\n",
    "\n",
    "INPUT_CANDS = \"outputs/candidates_val.jsonl\"\n",
    "OUT_SCORES = \"outputs/verification_scores_val.jsonl\"\n",
    "\n",
    "_rouge = hf_evaluate.load(\"rouge\")\n",
    "def rouge_l(hyp: str, ref: str) -> float:\n",
    "    r = _rouge.compute(predictions=[hyp], references=[ref])\n",
    "    return float(r[\"rougeL\"])\n",
    "\n",
    "total = 0\n",
    "Path(OUT_SCORES).parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(INPUT_CANDS, \"r\", encoding=\"utf-8\") as fin, open(OUT_SCORES, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in tqdm(fin, desc=\"Scoring candidates\"):\n",
    "        ex = json.loads(line)\n",
    "        src = ex[\"source\"]\n",
    "        cand = ex[\"candidate\"]\n",
    "        ref = ex[\"reference\"]\n",
    "        fscore = factcc_style_score(src, cand, chunk_tokens=350)\n",
    "        rec = {\n",
    "            \"id\": ex[\"id\"],\n",
    "            \"strategy\": ex[\"strategy\"],\n",
    "            \"params\": ex[\"params\"],\n",
    "            \"candidate\": cand,\n",
    "            \"reference\": ref,\n",
    "            \"factcc\": fscore,\n",
    "            \"rougeL\": rouge_l(cand, ref),\n",
    "            \"length\": ex[\"length\"],\n",
    "            \"logprob\": ex[\"logprob\"],\n",
    "        }\n",
    "        fout.write(json.dumps(rec) + \"\\n\")\n",
    "        total += 1\n",
    "\n",
    "print(f\"Wrote {total} records to {OUT_SCORES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c026d",
   "metadata": {},
   "source": [
    "**How to read results:**  \n",
    "You’ll see a progress bar as each candidate is scored. The output JSONL now contains `factcc` and `rougeL` per candidate, which we’ll aggregate next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b304c3",
   "metadata": {},
   "source": [
    "## What / Why\n",
    "**What:** Compare three selectors per article: **Baseline** (beam from Notebook 03), **Best‑of‑K by model log‑prob**, and **Reranked by FactCC**.  \n",
    "**Why:** Our success criterion is **≥ +2.0 FactCC** over baseline with **≤ 1.0 ROUGE‑L drop**. We’ll compute per‑article deltas and summary statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statistics import mean\n",
    "\n",
    "by_id = defaultdict(list)\n",
    "with open(OUT_SCORES, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        ex = json.loads(line)\n",
    "        by_id[ex[\"id\"]].append(ex)\n",
    "\n",
    "def select_baseline(cands):\n",
    "    beams = [c for c in cands if c[\"strategy\"] == \"diverse_beam\"]\n",
    "    if beams:\n",
    "        return max(beams, key=lambda x: x[\"logprob\"])\n",
    "    return max(cands, key=lambda x: x[\"logprob\"])\n",
    "\n",
    "def select_best_of_k(cands):\n",
    "    return max(cands, key=lambda x: x[\"logprob\"])\n",
    "\n",
    "def select_reranked_factcc(cands):\n",
    "    return max(cands, key=lambda x: x[\"factcc\"])\n",
    "\n",
    "rows = []\n",
    "for i, cands in by_id.items():\n",
    "    base = select_baseline(cands)\n",
    "    bestk = select_best_of_k(cands)\n",
    "    rerank = select_reranked_factcc(cands)\n",
    "    rows.append({\n",
    "        \"id\": i,\n",
    "        \"baseline_factcc\": base[\"factcc\"],\n",
    "        \"baseline_rougeL\": base[\"rougeL\"],\n",
    "        \"bestk_factcc\": bestk[\"factcc\"],\n",
    "        \"bestk_rougeL\": bestk[\"rougeL\"],\n",
    "        \"rerank_factcc\": rerank[\"factcc\"],\n",
    "        \"rerank_rougeL\": rerank[\"rougeL\"],\n",
    "    })\n",
    "\n",
    "def avg(col): return mean(r[col] for r in rows)\n",
    "\n",
    "summary = {\n",
    "    \"n_articles\": len(rows),\n",
    "    \"baseline\": {\"factcc\": avg(\"baseline_factcc\"), \"rougeL\": avg(\"baseline_rougeL\")},\n",
    "    \"best_of_k\": {\"factcc\": avg(\"bestk_factcc\"), \"rougeL\": avg(\"bestk_rougeL\")},\n",
    "    \"reranked\": {\"factcc\": avg(\"rerank_factcc\"), \"rougeL\": avg(\"rerank_rougeL\")},\n",
    "    \"deltas_vs_baseline\": {\n",
    "        \"best_of_k\": {\n",
    "            \"factcc\": avg(\"bestk_factcc\") - avg(\"baseline_factcc\"),\n",
    "            \"rougeL\": avg(\"bestk_rougeL\") - avg(\"baseline_rougeL\"),\n",
    "        },\n",
    "        \"reranked\": {\n",
    "            \"factcc\": avg(\"rerank_factcc\") - avg(\"baseline_factcc\"),\n",
    "            \"rougeL\": avg(\"rerank_rougeL\") - avg(\"baseline_rougeL\"),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "Path(\"outputs\").mkdir(parents=True, exist_ok=True)\n",
    "with open(\"outputs/rerank_summary_val.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(json.dumps(summary, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a73acc",
   "metadata": {},
   "source": [
    "**How to read results:**  \n",
    "Focus on `deltas_vs_baseline.reranked.factcc` (target: **≥ +2.0**) and `deltas_vs_baseline.reranked.rougeL` (target: **≥ −1.0** tolerance). If ROUGE‑L drops too much, we’ll tune decoding or apply constrained decoding as a fallback in ablations later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3665d627",
   "metadata": {},
   "source": [
    "## (Optional) What / Why\n",
    "**What:** Save **best‑per‑article** selections for human review and future stats.  \n",
    "**Why:** We’ll need these for the 50‑item human audit and bootstrap tests later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851930d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BEST_OUT = \"outputs/reranked_val_selections.jsonl\"\n",
    "with open(BEST_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i, cands in by_id.items():\n",
    "        base = max([c for c in cands if c[\"strategy\"] == \"diverse_beam\"], key=lambda x: x[\"logprob\"], default=max(cands, key=lambda x: x[\"logprob\"]))\n",
    "        rerank = max(cands, key=lambda x: x[\"factcc\"])\n",
    "        rec = {\n",
    "            \"id\": i,\n",
    "            \"baseline_candidate\": base[\"candidate\"],\n",
    "            \"reranked_candidate\": rerank[\"candidate\"],\n",
    "            \"reference\": base[\"reference\"],\n",
    "            \"baseline_factcc\": base[\"factcc\"],\n",
    "            \"reranked_factcc\": rerank[\"factcc\"],\n",
    "            \"baseline_rougeL\": base[\"rougeL\"],\n",
    "            \"reranked_rougeL\": rerank[\"rougeL\"],\n",
    "        }\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "print(f\"Wrote selections to {BEST_OUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f979a0a3",
   "metadata": {},
   "source": [
    "**How to read results:**  \n",
    "You now have a compact file to sample 50 items for the human audit. The JSONL stores both candidates and the reference for quick rubric‑based checks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
