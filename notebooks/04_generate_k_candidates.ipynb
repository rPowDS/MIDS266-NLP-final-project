{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e8a5988",
   "metadata": {},
   "source": [
    "# 04 — Candidate Generation (K per article)\n",
    "\n",
    "**Goal:** For each article, generate **K diverse summary candidates** and persist them with their **decoding params, lengths, and log‑probs**.  \n",
    "We’ll support: **diverse beam search**, **top‑k sampling**, and **top‑p (nucleus) sampling**.\n",
    "\n",
    "Format: **What/Why → Code (commented) → How to read results**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed4f5fd",
   "metadata": {},
   "source": [
    "## What / Why\n",
    "**What:** Imports, consistent config, and load the trained baseline checkpoint.  \n",
    "**Why:** We need the **same dataset split** and tokenization limits as training, then load our best baseline checkpoint to generate candidates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a65b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, subprocess\n",
    "def pip_install(pkgs):\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs, check=True)\n",
    "\n",
    "pip_install([\n",
    "    \"transformers==4.41.2\",\n",
    "    \"datasets==2.19.1\",\n",
    "    \"evaluate==0.4.2\",\n",
    "    \"accelerate==0.30.1\",\n",
    "    \"sentencepiece==0.1.99\",\n",
    "])\n",
    "\n",
    "import os, json, math, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM, set_seed)\n",
    "\n",
    "with open(\"configs/baseline.json\", \"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "SEED = cfg.get(\"seed\", 42)\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "set_seed(SEED)\n",
    "\n",
    "CKPT_DIR = Path(cfg[\"training\"][\"output_dir\"])\n",
    "assert CKPT_DIR.exists(), f\"Checkpoint dir not found: {CKPT_DIR}. Run notebook 03 first.\"\n",
    "\n",
    "raw = load_dataset(cfg[\"dataset\"][\"hf_id\"], cfg[\"dataset\"][\"config\"])\n",
    "tok = AutoTokenizer.from_pretrained(cfg[\"model\"][\"hf_id\"], use_fast=True)\n",
    "val = raw[cfg[\"dataset\"][\"split_val\"]]\n",
    "\n",
    "print(f\"Using checkpoint at: {CKPT_DIR.resolve()}\")\n",
    "print(val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271e8fb",
   "metadata": {},
   "source": [
    "**How to read results:**  \n",
    "If the checkpoint path exists and the validation split prints with length, you’re set. Use validation for dev; later you can run on the full test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520c9da",
   "metadata": {},
   "source": [
    "## What / Why\n",
    "**What:** Define **helper functions** to (1) tokenize a batch of sources, (2) generate multiple candidates per example with *different strategies*, and (3) compute approximate **log‑probabilities** for each output.  \n",
    "**Why:** We need *comparable ingredients* for reranking: the text, its length, and its model log‑probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72308427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict, Any\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(str(CKPT_DIR)).to(device)\n",
    "model.eval()\n",
    "\n",
    "SRC_COL = cfg[\"text_fields\"][\"source\"]\n",
    "REF_COL = cfg[\"text_fields\"][\"summary\"]\n",
    "MAX_SRC = cfg[\"tokenization\"][\"max_source_len\"]\n",
    "MAX_TGT = cfg[\"tokenization\"][\"max_target_len\"]\n",
    "\n",
    "def tokenize_sources(texts: List[str]):\n",
    "    batch = tok(\n",
    "        texts,\n",
    "        max_length=MAX_SRC,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "def sequence_logprob(output_ids: torch.Tensor, scores: List[torch.Tensor]) -> float:\n",
    "    # Convert scores to log-prob distribution and accumulate per generated token\n",
    "    logprobs = [torch.log_softmax(s, dim=-1) for s in scores]\n",
    "    gen_token_ids = output_ids[-len(scores):]\n",
    "    lp = 0.0\n",
    "    for t, tok_id in enumerate(gen_token_ids):\n",
    "        lp += float(logprobs[t][0, tok_id.item()].detach().cpu())\n",
    "    return lp\n",
    "\n",
    "def generate_candidates(inputs: Dict[str, torch.Tensor], strategy: str, num_return_sequences: int, common_kwargs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    if strategy == \"diverse_beam\":\n",
    "        kwargs = dict(num_beams=max(4, num_return_sequences),\n",
    "                      num_beam_groups=max(2, num_return_sequences//2),\n",
    "                      diversity_penalty=1.0,\n",
    "                      do_sample=False)\n",
    "    elif strategy == \"topk_sampling\":\n",
    "        kwargs = dict(do_sample=True, top_k=50, temperature=1.0, num_beams=1)\n",
    "    elif strategy == \"topp_sampling\":\n",
    "        kwargs = dict(do_sample=True, top_p=0.92, temperature=1.0, num_beams=1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_length=MAX_TGT,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        **kwargs,\n",
    "        **common_kwargs\n",
    "    )\n",
    "    decoded = tok.batch_decode(out.sequences, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    seqs = out.sequences\n",
    "    scores = out.scores\n",
    "\n",
    "    per_seq_logp = []\n",
    "    for i in range(seqs.shape[0]):\n",
    "        per_time = [s[i:i+1, :] for s in scores]\n",
    "        lp = sequence_logprob(seqs[i], per_time)\n",
    "        per_seq_logp.append(lp)\n",
    "\n",
    "    return {\"texts\": decoded, \"logprobs\": per_seq_logp}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab75a4f",
   "metadata": {},
   "source": [
    "**How to read results:**  \n",
    "Above, we defined *three strategies* and a function that returns generated texts and an **approximate sequence log‑probability** using the per‑step generation scores. This log‑prob is a good baseline for “best‑of‑K by the model.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4422dc",
   "metadata": {},
   "source": [
    "## What / Why\n",
    "**What:** Run generation over the validation set (or a slice) and persist **JSONL** with per‑candidate metadata.  \n",
    "**Why:** Reranking needs a persistent, analyzable artifact: `{id, source, reference, candidate, strategy, params, length, logprob}`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "\n",
    "GEN_CFG = {\n",
    "    \"split\": \"validation\",\n",
    "    \"max_items\": 500,\n",
    "    \"K_per_strategy\": 4,\n",
    "    \"common_kwargs\": {\"length_penalty\": 1.0, \"no_repeat_ngram_size\": 3},\n",
    "    \"strategies\": [\"diverse_beam\", \"topk_sampling\", \"topp_sampling\"],\n",
    "    \"output_jsonl\": \"outputs/candidates_val.jsonl\",\n",
    "}\n",
    "\n",
    "Path(\"outputs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ds = raw[GEN_CFG[\"split\"]]\n",
    "N = len(ds) if GEN_CFG[\"max_items\"] is None else min(GEN_CFG[\"max_items\"], len(ds))\n",
    "\n",
    "with open(GEN_CFG[\"output_jsonl\"], \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx in tqdm(range(N), total=N):\n",
    "        ex = ds[idx]\n",
    "        src = ex[SRC_COL]\n",
    "        ref = ex[REF_COL]\n",
    "        batch_inputs = tokenize_sources([src])\n",
    "        for strat in GEN_CFG[\"strategies\"]:\n",
    "            results = generate_candidates(\n",
    "                batch_inputs,\n",
    "                strategy=strat,\n",
    "                num_return_sequences=GEN_CFG[\"K_per_strategy\"],\n",
    "                common_kwargs=GEN_CFG[\"common_kwargs\"],\n",
    "            )\n",
    "            for txt, lp in zip(results[\"texts\"], results[\"logprobs\"]):\n",
    "                rec = {\n",
    "                    \"id\": idx,\n",
    "                    \"source\": src,\n",
    "                    \"reference\": ref,\n",
    "                    \"candidate\": txt,\n",
    "                    \"strategy\": strat,\n",
    "                    \"params\": {\n",
    "                        **GEN_CFG[\"common_kwargs\"],\n",
    "                        \"strategy\": strat,\n",
    "                        \"K_per_strategy\": GEN_CFG[\"K_per_strategy\"],\n",
    "                    },\n",
    "                    \"length\": len(tok.encode(txt, add_special_tokens=False)),\n",
    "                    \"logprob\": lp,\n",
    "                }\n",
    "                f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote candidates to: {GEN_CFG['output_jsonl']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d572f842",
   "metadata": {},
   "source": [
    "**How to read results:**  \n",
    "You’ll get a growing `outputs/candidates_val.jsonl`. Each line captures one candidate with its **strategy, params, length, and log‑prob**. This file is the input to the verification/reranking step.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
