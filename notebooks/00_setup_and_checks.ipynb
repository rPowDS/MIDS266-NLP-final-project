{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 — Setup & Sanity Checks\n",
    "\n",
    "**What/Why**: Establish a *known‑good* environment (pinned versions), print system info, seed all RNGs, and validate dataset access.\n",
    "\n",
    "You can run this on Colab (GPU recommended). Each code cell includes **line‑by‑line comments** and a **How to read results** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install pinned dependencies (Colab)\n",
    "Skip if you already created a virtualenv locally and installed `requirements.txt`. On Colab, uncomment and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% If running on Google Colab, uncomment the next lines\n",
    "# !pip install -q transformers==4.41.2 datasets==2.20.0 evaluate==0.4.2 accelerate==0.31.0\n",
    "# !pip install -q torch>=2.1 numpy==1.26.4 pandas==2.1.4 scipy==1.11.4 tqdm==4.66.4\n",
    "# !pip install -q nltk==3.8.1 typer==0.12.3 rich==13.7.1 bert-score==0.3.13 rouge-score==0.1.2\n",
    "# !pip install -q sentencepiece==0.2.0 sacrebleu==2.4.0 huggingface-hub==0.24.5 orjson==3.10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & environment summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os, sys, json, random, time, platform, textwrap, pathlib\n",
    "\n",
    "# Core libs\n",
    "import numpy as np\n",
    "\n",
    "# Hugging Face ecosystem\n",
    "import datasets, transformers, evaluate\n",
    "\n",
    "# Other tools\n",
    "import nltk, torch\n",
    "\n",
    "# Print versions for reproducibility\n",
    "print('Python  :', sys.version.split()[0])\n",
    "print('Platform:', platform.platform())\n",
    "print('PyTorch :', torch.__version__)\n",
    "print('Transformers:', transformers.__version__)\n",
    "print('Datasets:', datasets.__version__)\n",
    "print('Evaluate:', evaluate.__version__)\n",
    "print('NLTK    :', nltk.__version__)\n",
    "\n",
    "# How to read results:\n",
    "# This shows the exact versions used in this session. Copy these to your report's appendix and logs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create folders & global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standard folders (no error if they already exist)\n",
    "for d in ['configs', 'runs', 'results']:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Fixed seeds for reproducibility (HF Trainer will also receive this seed)\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Create a tiny run config to track settings consistently\n",
    "run_cfg = {\n",
    "    'seed': SEED,\n",
    "    'dataset_id': 'cnn_dailymail',\n",
    "    'dataset_repo': 'ccdv/cnn_dailymail',\n",
    "    'dataset_config': '3.0.0',\n",
    "    'baseline_model': 'facebook/bart-base',\n",
    "    'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "}\n",
    "with open('configs/run.json', 'w') as f:\n",
    "    json.dump(run_cfg, f, indent=2)\n",
    "\n",
    "print('Wrote configs/run.json')\n",
    "\n",
    "# How to read results:\n",
    "# 'configs/run.json' records critical identifiers (dataset, model, seed). We will append to it later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NLTK data (tokenizers, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use NLTK for tokenization in some metrics. This downloads small helper data.\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)  # fallback for newer nltk tokenizers\n",
    "print('Downloaded NLTK data.')\n",
    "\n",
    "# How to read results:\n",
    "# If downloads fail (offline), you can run metrics that don't require NLTK, or re-run with internet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CNN/DailyMail (config 3.0.0) and show 3 redacted examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset (default split names: train/validation/test)\n",
    "ds = load_dataset('cnn_dailymail', '3.0.0')\n",
    "\n",
    "# Redact named entities or digits in previews to prevent accidental PIIs (simple demo)\n",
    "import re\n",
    "def redact(text):\n",
    "    text = re.sub(r'\\b[A-Z][a-z]+\\b', '[NAME]', text)\n",
    "    text = re.sub(r'\\d', '0', text)\n",
    "    return text\n",
    "\n",
    "for i in range(3):\n",
    "    art = ds['train'][i]['article'][:400].replace('\\n', ' ')\n",
    "    summ = ds['train'][i]['highlights'][:200].replace('\\n', ' ')\n",
    "    print(f'Example {i+1}\\nArticle  :', redact(art))\n",
    "    print('Summary :', redact(summ))\n",
    "    print('-'*80)\n",
    "\n",
    "# Save small samples so later notebooks can run quick dry-runs\n",
    "os.makedirs('results', exist_ok=True)\n",
    "ds['validation'].select(range(64)).to_json('results/val.sample.jsonl', orient='records', lines=True)\n",
    "print('Wrote results/val.sample.jsonl (64 items).')\n",
    "\n",
    "# How to read results:\n",
    "# You should see 3 preview examples with simple redactions. The small validation sample enables quick experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Record dataset license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the dataset license from the Hugging Face dataset card.\n",
    "cfg = json.load(open('configs/run.json'))\n",
    "cfg['dataset_license'] = 'Apache-2.0'\n",
    "json.dump(cfg, open('configs/run.json','w'), indent=2)\n",
    "print(\"Updated configs/run.json with dataset license 'Apache-2.0'.\")\n",
    "\n",
    "# How to read results:\n",
    "# The generated configs/run.json now includes the dataset's Apache-2.0 license for audit trails.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "00_setup_and_checks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
