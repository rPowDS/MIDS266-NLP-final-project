{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92bf195e",
   "metadata": {},
   "source": [
    "# 03 — Baseline Fine‑Tuning (BART‑base on CNN/DailyMail)\n",
    "\n",
    "**Project:** Reducing Hallucinations via Verifier‑Reranking  \n",
    "**Dataset:** `ccdv/cnn_dailymail` (config `3.0.0`)  \n",
    "**Model:** `facebook/bart-base`\n",
    "\n",
    "This notebook fine‑tunes a BART‑base summarizer and establishes the baseline metrics we will later compare to reranking.\n",
    "Each step is structured as **What/Why → Code (commented) → How to read results**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e63bb16",
   "metadata": {},
   "source": [
    "### Reference Context\n",
    "This notebook reproduces the BART fine-tuning recipe from Lewis et al. (2020) on the CNN/DailyMail summarization benchmark (Hermann et al., 2015; See et al., 2017). The model minimizes the conditional log-loss\n",
    "\\[\n",
    "\\mathcal{L}(\\theta) = - \\sum_{t=1}^{T} \\log p_\\theta(y_t \\mid y_{<t}, x),\n",
    "\\]\n",
    "using teacher forcing, where \\(x\\) denotes the article and \\(y_t\\) the reference highlight tokens. Metrics reported later (ROUGE, BERTScore) follow the evaluation protocol from these works.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a7c61",
   "metadata": {},
   "source": [
    "## What / Why\n",
    "**What:** Install exact library versions and import dependencies.  \n",
    "**Why:** Reproducibility requires **pinned versions** so runs on Colab or locally behave the same. We'll also set the global random seed to make results comparable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e67f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install pinned versions for reproducibility\n",
    "import sys, subprocess\n",
    "\n",
    "def pip_install(requirements):\n",
    "    # Install packages with specific versions\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + requirements\n",
    "    print(\"Installing:\", \" \".join(requirements))\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "REQ = [\n",
    "    \"transformers==4.41.2\",\n",
    "    \"datasets==2.19.1\",\n",
    "    \"evaluate==0.4.2\",\n",
    "    \"rouge-score==0.1.2\",\n",
    "    \"bert-score==0.3.13\",\n",
    "    \"accelerate==0.30.1\",\n",
    "    \"sentencepiece==0.1.99\",\n",
    "    \"sacrebleu==2.4.0\",\n",
    "]\n",
    "pip_install(REQ)\n",
    "\n",
    "# Imports\n",
    "import os, math, random, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "                          DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
    "                          set_seed)\n",
    "import evaluate\n",
    "\n",
    "# Seed everything for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "set_seed(SEED)\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA:\", torch.version.cuda if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"Device:\", \"GPU\" if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa208f",
   "metadata": {},
   "source": [
    "**How to read results:**  \n",
    "If the cell installed packages without errors and printed versions and device info, you're ready to continue. The *seed* is set, so repeated runs with the same data and hyper‑parameters are comparable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186eceff",
   "metadata": {},
   "source": [
    "## What / Why\n",
    "**What:** Define one **central config** (dataset/model IDs, max lengths, training hyper‑parameters, decoding params).  \n",
    "**Why:** Keeping *all knobs in one dictionary* avoids buried magic numbers and lets graders reproduce settings precisely. We'll also save this to disk as `configs/baseline.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8fa918",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "cfg = {\n",
    "    \"project\": \"verifier_reranking_cnndm\",\n",
    "    \"timestamp_utc\": datetime.utcnow().isoformat(timespec=\"seconds\"),\n",
    "    \"seed\": 42,\n",
    "    \"dataset\": {\"hf_id\": \"ccdv/cnn_dailymail\", \"config\": \"3.0.0\", \"split_train\": \"train\", \"split_val\": \"validation\", \"split_test\": \"test\"},\n",
    "    \"text_fields\": {\"source\": \"article\", \"summary\": \"highlights\"},\n",
    "    \"model\": {\"hf_id\": \"facebook/bart-base\"},\n",
    "    \"tokenization\": {\"max_source_len\": 512, \"max_target_len\": 128, \"truncation\": True},\n",
    "    \"training\": {\n",
    "        \"output_dir\": \"checkpoints/bart-base-cnndm\",\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"per_device_train_batch_size\": 8,\n",
    "        \"per_device_eval_batch_size\": 8,\n",
    "        \"gradient_accumulation_steps\": 2,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"warmup_ratio\": 0.03,\n",
    "        \"lr_scheduler_type\": \"linear\",\n",
    "        \"logging_steps\": 100,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"eval_steps\": 1000,\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"save_steps\": 1000,\n",
    "        \"predict_with_generate\": True,\n",
    "        \"generation_max_length\": 128,\n",
    "        \"generation_num_beams\": 4,\n",
    "        \"report_to\": \"none\"\n",
    "    },\n",
    "    \"decoding\": {\"num_beams\": 4, \"length_penalty\": 1.0},\n",
    "    \"notes\": \"Baseline BART-base on CNN/DailyMail v3.0.0 with beam search decoding.\"\n",
    "}\n",
    "\n",
    "Path(\"configs\").mkdir(parents=True, exist_ok=True)\n",
    "with open(\"configs/baseline.json\", \"w\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "\n",
    "print(json.dumps(cfg, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b37ab6d",
   "metadata": {},
   "source": [
    "**How to read results:**  \n",
    "You should see the JSON config printed. The `output_dir` is where checkpoints and logs will be written. You can change `num_train_epochs` for longer training (e.g., 5) if you have more time/GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ce1c1",
   "metadata": {},
   "source": [
    "## What / Why\n",
    "**What:** Load the CNN/DailyMail dataset and show **three redacted examples**.  \n",
    "**Why:** Sanity‑check fields, and confirm our preprocessing won't leak sensitive tokens (we'll lightly redact digits/URLs). We keep only `article` and `highlights` (reference summary).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw = load_dataset(cfg[\"dataset\"][\"hf_id\"], cfg[\"dataset\"][\"config\"])\n",
    "print(raw)\n",
    "\n",
    "import re\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "DIGIT_RE = re.compile(r\"\\d\")\n",
    "\n",
    "def redact(text: str) -> str:\n",
    "    t = URL_RE.sub(\"[URL]\", text)\n",
    "    t = DIGIT_RE.sub(\"#\", t)\n",
    "    return t\n",
    "\n",
    "def show_samples(ds, n=3):\n",
    "    for i in range(n):\n",
    "        ex = ds[i]\n",
    "        src = redact(ex[cfg[\"text_fields\"][\"source\"]])[:600].strip()\n",
    "        ref = redact(ex[cfg[\"text_fields\"][\"summary\"]])[:300].strip()\n",
    "        print(f\"--- Example {i} ---\")\n",
    "        print(\"SOURCE:\\n\", src, \"\\n\")\n",
    "        print(\"REFERENCE SUMMARY:\\n\", ref, \"\\n\")\n",
    "\n",
    "print(\"\\nValidation samples (redacted):\")\n",
    "show_samples(raw[cfg[\"dataset\"][\"split_val\"]], n=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a64fb",
   "metadata": {},
   "source": [
    "**How to read results:**  \n",
    "You should see three validation examples with URLs replaced and digits masked. This verifies field names: `article` → source, `highlights` → reference summary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcab11a",
   "metadata": {},
   "source": [
    "## What / Why\n",
    "**What:** Tokenize and prepare dataset for sequence‑to‑sequence training.  \n",
    "**Why:** We must truncate/pad to model limits and place labels in the `text_target` slot so the Trainer can compute loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c4940",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tok = AutoTokenizer.from_pretrained(cfg[\"model\"][\"hf_id\"], use_fast=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(cfg[\"model\"][\"hf_id\"])\n",
    "\n",
    "def preprocess(batch):\n",
    "    sources = batch[cfg[\"text_fields\"][\"source\"]]\n",
    "    targets = batch[cfg[\"text_fields\"][\"summary\"]]\n",
    "    model_inputs = tok(\n",
    "        sources,\n",
    "        max_length=cfg[\"tokenization\"][\"max_source_len\"],\n",
    "        truncation=cfg[\"tokenization\"][\"truncation\"],\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    with tok.as_target_tokenizer():\n",
    "        labels = tok(\n",
    "            targets,\n",
    "            max_length=cfg[\"tokenization\"][\"max_target_len\"],\n",
    "            truncation=cfg[\"tokenization\"][\"truncation\"],\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "cols_to_keep = [cfg[\"text_fields\"][\"source\"], cfg[\"text_fields\"][\"summary\"]]\n",
    "proc_train = raw[cfg[\"dataset\"][\"split_train\"]].remove_columns(\n",
    "    [c for c in raw[\"train\"].column_names if c not in cols_to_keep]\n",
    ").map(preprocess, batched=True, remove_columns=cols_to_keep)\n",
    "\n",
    "proc_val = raw[cfg[\"dataset\"][\"split_val\"]].remove_columns(\n",
    "    [c for c in raw[\"validation\"].column_names if c not in cols_to_keep]\n",
    ").map(preprocess, batched=True, remove_columns=cols_to_keep)\n",
    "\n",
    "proc_test = raw[cfg[\"dataset\"][\"split_test\"]].remove_columns(\n",
    "    [c for c in raw[\"test\"].column_names if c not in cols_to_keep]\n",
    ").map(preprocess, batched=True, remove_columns=cols_to_keep)\n",
    "\n",
    "print(proc_train, proc_val, proc_test, sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aadf00",
   "metadata": {},
   "source": [
    "**How to read results:**  \n",
    "The printed dataset objects should now have only numeric tensors (`input_ids`, `attention_mask`, `labels`). That means they’re ready for the Trainer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81af6d46",
   "metadata": {},
   "source": [
    "## What / Why\n",
    "**What:** Define collator and ROUGE metric function for evaluation.  \n",
    "**Why:** The **data collator** pads to a multiple of 8 tokens for faster tensor cores on GPU. ROUGE gives a baseline string‑overlap view of summary quality; we'll compute BERTScore only once after training to save time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7693b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer=tok, model=model, pad_to_multiple_of=8)\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "def compute_rouge(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    def decode(seqs):\n",
    "        return [tok.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in seqs]\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    preds_text = decode(preds)\n",
    "    labels[labels == -100] = tok.pad_token_id\n",
    "    refs_text = decode(labels)\n",
    "    result = rouge.compute(predictions=preds_text, references=refs_text, use_stemmer=True)\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5213bf",
   "metadata": {},
   "source": [
    "**How to read results:**  \n",
    "Nothing printed here; we just prepared the metric function. We’ll see ROUGE during/after evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60dc731",
   "metadata": {},
   "source": [
    "## What / Why\n",
    "**What:** Configure `Seq2SeqTrainer` and launch fine‑tuning.  \n",
    "**Why:** Trainer handles batching, gradient accumulation, mixed precision, checkpointing, and evaluation for us — all with reproducible arguments saved in our config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5ef2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=cfg[\"training\"][\"output_dir\"],\n",
    "    num_train_epochs=cfg[\"training\"][\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=cfg[\"training\"][\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=cfg[\"training\"][\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=cfg[\"training\"][\"gradient_accumulation_steps\"],\n",
    "    learning_rate=cfg[\"training\"][\"learning_rate\"],\n",
    "    weight_decay=cfg[\"training\"][\"weight_decay\"],\n",
    "    warmup_ratio=cfg[\"training\"][\"warmup_ratio\"],\n",
    "    lr_scheduler_type=cfg[\"training\"][\"lr_scheduler_type\"],\n",
    "    logging_steps=cfg[\"training\"][\"logging_steps\"],\n",
    "    evaluation_strategy=cfg[\"training\"][\"evaluation_strategy\"],\n",
    "    eval_steps=cfg[\"training\"][\"eval_steps\"],\n",
    "    save_strategy=cfg[\"training\"][\"save_strategy\"],\n",
    "    save_steps=cfg[\"training\"][\"save_steps\"],\n",
    "    predict_with_generate=cfg[\"training\"][\"predict_with_generate\"],\n",
    "    generation_max_length=cfg[\"training\"][\"generation_max_length\"],\n",
    "    generation_num_beams=cfg[\"training\"][\"generation_num_beams\"],\n",
    "    report_to=cfg[\"training\"][\"report_to\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=proc_train,\n",
    "    eval_dataset=proc_val,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_rouge,\n",
    ")\n",
    "\n",
    "train_output = trainer.train()\n",
    "print(train_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e237f",
   "metadata": {},
   "source": [
    "**How to read results:**  \n",
    "Watch the logs: you’ll see training steps and periodic evaluation with ROUGE scores. After it finishes, the final checkpoint is in `checkpoints/bart-base-cnndm`. For a quick smoke test, you can set `num_train_epochs=1` first, then increase later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211f9bb",
   "metadata": {},
   "source": [
    "## What / Why\n",
    "**What:** Evaluate on validation and test with **beam search decoding** and compute **ROUGE + BERTScore**.  \n",
    "**Why:** These are our baseline quality metrics to compare against reranking later. We report ROUGE‑1/2/L and BERTScore (F1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fd70da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def summarize_split(dataset, split_name: str, out_jsonl: str):\n",
    "    preds = trainer.predict(dataset, max_length=cfg[\"decoding\"][\"generation_max_length\"] if \"generation_max_length\" in cfg[\"training\"] else 128)\n",
    "    def decode(seqs):\n",
    "        return [tok.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in seqs]\n",
    "    pred_texts = decode(preds.predictions)\n",
    "    labels = preds.label_ids\n",
    "    labels[labels == -100] = tok.pad_token_id\n",
    "    ref_texts = decode(labels)\n",
    "\n",
    "    rouge_scores = rouge.compute(predictions=pred_texts, references=ref_texts, use_stemmer=True)\n",
    "    rouge_scores = {k: round(v, 4) for k, v in rouge_scores.items()}\n",
    "    print(f\"[{split_name}] ROUGE:\", rouge_scores)\n",
    "\n",
    "    bertscore = evaluate.load(\"bertscore\")\n",
    "    bs = bertscore.compute(predictions=pred_texts, references=ref_texts, lang=\"en\")\n",
    "    bert_f1 = float(np.mean(bs[\"f1\"]))\n",
    "    print(f\"[{split_name}] BERTScore F1: {bert_f1:.4f}\")\n",
    "\n",
    "    Path(\"outputs\").mkdir(exist_ok=True, parents=True)\n",
    "    out_path = Path(\"outputs\") / out_jsonl\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for hyp, ref in zip(pred_texts, ref_texts):\n",
    "            f.write(json.dumps({\"prediction\": hyp, \"reference\": ref}) + \"\\n\")\n",
    "    print(f\"Saved {len(pred_texts)} predictions to {out_path}\")\n",
    "\n",
    "    return {\"rouge\": rouge_scores, \"bertscore_f1\": bert_f1}\n",
    "\n",
    "val_metrics = summarize_split(proc_val, \"val\", \"baseline_val_predictions.jsonl\")\n",
    "test_metrics = summarize_split(proc_test.select(range(2000)), \"test_subset\", \"baseline_test_subset_predictions.jsonl\")\n",
    "print(\"VAL:\", val_metrics)\n",
    "print(\"TEST (subset):\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8cbabf",
   "metadata": {},
   "source": [
    "**How to read results:**  \n",
    "You’ll see ROUGE and BERTScore for validation and a test *subset* (the full test can be large; use the subset to save time). These numbers are your **baseline**. We saved predictions in `outputs/` for downstream use.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
