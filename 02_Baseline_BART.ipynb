{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyNFA7RSxnf42WbAcS+9K6+9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Background & Rationale\n","\n","### Why BART?\n","\n"," **BART-base** (Lewis et al., 2020)  for several reasons:\n","\n","1. **Architecture Fit**: BART as a encoder-decoder transformer pre-trained with a denoising objective, its well-suited for sequence-to-sequence tasks like summarization.\n","\n","2. **Established Baseline**: BART achieves strong performance on CNN/DailyMail (Lewis et al., 2020 report ROUGE-L ~40 for BART-large). Using BART-base provides a fair baseline within the GPU compute constraints.\n","\n","3. **Reproducibility**: The `facebook/bart-base` checkpoint is publicly available via Hugging Face, ensuring reproducibility.\n","\n","4. **Compatibility**: BART's architecture allows for multiple candidates via beam search, which is essential for the reranking approach.\n","\n","**Alternative Considered**: PEGASUS (Zhang et al., 2020) achieves higher ROUGE on CNN/DailyMail but requires more compute. T5 (Raffel et al., 2019) is another option but BART's denoising pre-training is more aligned with summarization.\n","\n","### Training Objective\n","\n","By fine-tuning BART using the standard **autoregressive cross-entropy loss**:\n","\n","$$\\mathcal{L}(\\theta) = -\\sum_{t=1}^{T} \\log P_\\theta(y_t \\mid y_{<t}, x)$$\n","\n","Where:\n","- $x$ = source article (encoder input)\n","- $y$ = target summary (decoder output)\n","- $\\theta$ = model parameters\n","- $T$ = summary length\n","\n","This objective teaches the model to predict each summary token given the article and all previous summary tokens.\n"],"metadata":{"id":"4cCULJO65lF8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZX4UU77f5hSZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763176783213,"user_tz":480,"elapsed":16332,"user":{"displayName":"R Powers","userId":"06683789047320527470"}},"outputId":"08a7e45b-67c9-432a-ef96-90076d062670","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Loaded config from: /content/drive/MyDrive/w266_project_final/configs/baseline.json\n","Global random seed set to 42.\n","Using device: cuda\n","GPU Name: NVIDIA A100-SXM4-40GB\n"]}],"source":["import os\n","import json\n","import random\n","import pathlib\n","import numpy as np\n","import torch\n","from datasets import load_dataset, load_from_disk\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSeq2SeqLM,\n","    DataCollatorForSeq2Seq,\n","    Seq2SeqTrainer,\n","    Seq2SeqTrainingArguments,\n","    set_seed\n",")\n","\n","!pip install evaluate\n","import evaluate\n","import nltk\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Load Central Config\n","BASE_DIR = \"/content/drive/MyDrive/w266_project_final\"\n","CONFIGS_DIR = os.path.join(BASE_DIR, \"configs\")\n","CONFIG_PATH = os.path.join(CONFIGS_DIR, \"baseline.json\")\n","\n","with open(CONFIG_PATH, 'r') as f:\n","    cfg = json.load(f)\n","print(f\"Loaded config from: {CONFIG_PATH}\")\n","\n","# Set Seed (Guardrail #5)\n","SEED = cfg['seed']\n","set_seed(SEED)\n","print(f\"Global random seed set to {SEED}.\")\n","\n","# Define Artifact Paths\n","DATA_DIR = os.path.join(BASE_DIR, \"data\")\n","MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n","RESULTS_DIR = os.path.join(BASE_DIR, \"results\")\n","\n","HF_CACHE_DIR = os.path.join(DATA_DIR, \"hf_cache\")\n","SNAPSHOT_DIR = os.path.join(DATA_DIR, \"snapshots/cnndm_tok_bart_base\")\n","CHECKPOINT_DIR = os.path.join(BASE_DIR, cfg['train']['output_dir'])\n","\n","for d in [HF_CACHE_DIR, SNAPSHOT_DIR, CHECKPOINT_DIR, RESULTS_DIR]:\n","    pathlib.Path(d).mkdir(parents=True, exist_ok=True)\n","os.environ[\"HF_HOME\"] = HF_CACHE_DIR\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using device: {device}\")\n","if device == \"cuda\":\n","    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"]},{"cell_type":"markdown","source":["# 2. Data Preprocessing What / Why: - Baseline Training (Guardrail #4)\n","\n","##What: Fine-tuning BART using a standard cross-entropy loss.\n","##Why: This creates \"Baseline B\" (Guardrail #2).\n","\n","This follows the BART paper (Lewis et al., 2020).\n","\n","Tokenize the inputs using the BART tokenizer with truncation settings derived from EDA:\n","* **Source:** 1024 tokens (captures full article context).\n","* **Target:** 128 tokens (sufficient for abstractive summaries).\n","We also create a random subset (Train=20k, Val=2k) to ensure the project remains computationally feasible on Colab GPUs.\n","\n","\n","## Data Subset Justification\n","\n","### Why 20,000 Training Examples?\n","\n","The full CNN/DailyMail training set contains 287,113 examples. Using a **random subset of 20,000** for the following reasons:\n","\n","1. **Compute Constraints**: Fine-tuning on the full dataset would require ~15+ hours on a Colab GPU. Our 20K subset trains in far less time (3 epochs).\n","\n","2. **Sufficient Signal**: Prior work shows that transformer models can achieve competitive performance with smaller training sets when starting from pre-trained checkpoints (Howard & Ruder, 2018).\n","\n","3. **Reproducibility**: Using a fixed random seed (42) ensures that the subset is reproducible across runs.\n","\n","4. **Focus on Reranking**: The project proposal is attempting a reranking method, not achieving state-of-the-art generation. A competitive baseline is sufficient.\n","\n","**Potential Limitation**: Training on 20K examples may result in slightly lower ROUGE than training on the full dataset. However, test-set ROUGE-L (28.09) is within expected range for BART-base, confirming the baseline is valid.\n","\n","### Tokenization Settings\n","\n","| Parameter | Value | Justification |\n","|-----------|-------|---------------|\n","| `max_source_len` | 1024 | Covers 95%+ of articles without truncation (from EDA) |\n","| `max_target_len` | 128 | >2x the average summary length (~56 tokens); prevents truncation |\n","\n","These settings follow standard practice for CNN/DailyMail summarization (See et al., 2017; Lewis et al., 2020)."],"metadata":{"id":"TLU_71Yf6CuW"}},{"cell_type":"code","source":["# Load and Tokenize Dataset\n","\n","\n","tokenizer = AutoTokenizer.from_pretrained(cfg['model_name'], use_fast=True)\n","\n","MAX_INPUT = cfg['tokenization']['max_source_len']\n","MAX_TARGET = cfg['tokenization']['max_target_len']\n","SOURCE_COL = cfg['text_fields']['source']\n","SUMMARY_COL = cfg['text_fields']['summary']\n","\n","def preprocess(batch):\n","    model_inputs = tokenizer(batch[SOURCE_COL], max_length=MAX_INPUT, truncation=True, padding=False)\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(batch[SUMMARY_COL], max_length=MAX_TARGET, truncation=True, padding=False)\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","#  Load  Dataset\n","SNAPSHOT_PATH = os.path.join(DATA_DIR, \"snapshots/cnndm_tok_bart_base\")\n","if os.path.exists(SNAPSHOT_PATH) and os.path.exists(os.path.join(SNAPSHOT_PATH, \"dataset_dict.json\")):\n","    print(f\"Loading tokenized snapshot from disk: {SNAPSHOT_PATH}\")\n","    ds_tok = load_from_disk(SNAPSHOT_PATH)\n","else:\n","    print(f\"No snapshot found. Loading from Hugging Face and tokenizing...\")\n","    ds = load_dataset(cfg['dataset_name'], cfg['dataset_config'], cache_dir=HF_CACHE_DIR)\n","\n","    print(\"Tokenizing...\")\n","    ds_tok = ds.map(\n","        preprocess,\n","        batched=True,\n","        num_proc=os.cpu_count(),\n","        remove_columns=ds['train'].column_names\n","    )\n","\n","    print(f\"Saving tokenized snapshot to: {SNAPSHOT_PATH}\")\n","    ds_tok.save_to_disk(SNAPSHOT_PATH)\n","\n","ds_tok.set_format(type=\"torch\", columns=['input_ids', 'attention_mask', 'labels'])\n","\n","# Create Subsets\n","TRAIN_SUBSET_SIZE = cfg['train_subset_size']\n","EVAL_SUBSET_SIZE = cfg['val_subset_size']\n","\n","full_train = ds_tok['train'].shuffle(seed=SEED)\n","train_ds = full_train.select(range(TRAIN_SUBSET_SIZE))\n","\n","full_eval = ds_tok['validation'].shuffle(seed=SEED)\n","eval_ds = full_eval.select(range(EVAL_SUBSET_SIZE))\n","\n","print(\"\\n--- Tokenized Dataset (Using Subsets) ---\")\n","print(f\"Training on {len(train_ds)} examples (randomly sampled)\")\n","print(f\"Evaluating on {len(eval_ds)} examples (randomly sampled)\")"],"metadata":{"id":"UMEbjmdj5kvm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763176785148,"user_tz":480,"elapsed":1925,"user":{"displayName":"R Powers","userId":"06683789047320527470"}},"outputId":"645c756c-6c0d-4591-af6c-140844fbe275"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Loading tokenized snapshot from disk: /content/drive/MyDrive/w266_project_final/data/snapshots/cnndm_tok_bart_base\n","\n","--- Tokenized Dataset (Using Subsets) ---\n","Training on 20000 examples (randomly sampled)\n","Evaluating on 2000 examples (randomly sampled)\n"]}]},{"cell_type":"markdown","source":["## 3.0: Model Fine-Tuning\n","Using the Hugging Face `Seq2SeqTrainer` for optimized training.\n","* **Metric:** ROUGE-L is calculated at each epoch to monitor convergence.\n","* **Precision:** Mixed Precision (FP16) is enabled to speed up training on A100/T4 GPUs.\n","* **Saving:** The best model (lowest validation loss) is automatically saved."],"metadata":{"id":"_9bAFebMZv8P"}},{"cell_type":"code","source":["# Configure and Launch Trainer (Guardrail #5)\n","\n","#  Load Model and Data\n","model = AutoModelForSeq2SeqLM.from_pretrained(cfg['model_name']).to(device)\n","data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, pad_to_multiple_of=8)\n","\n","# Define ROUGE Metric for Evaluation (Guardrail #6)\n","!pip install rouge_score -q\n","import nltk\n","nltk.download('punkt', quiet=True)\n","nltk.download('punkt_tab', quiet=True)\n","import evaluate\n","rouge = evaluate.load(\"rouge\")\n","\n","\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple): preds = preds[0]\n","\n","    preds_copy = np.where(preds == -100, tokenizer.pad_token_id, preds)\n","    decoded_preds = tokenizer.batch_decode(preds_copy, skip_special_tokens=True)\n","\n","\n","    labels_copy = np.where(labels == -100, tokenizer.pad_token_id, labels)\n","    decoded_labels = tokenizer.batch_decode(labels_copy, skip_special_tokens=True)\n","\n","\n","    # Adding a newline after each sentence for ROUGE-Lsum\n","    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(p.strip())) for p in decoded_preds]\n","    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(l.strip())) for l in decoded_labels]\n","\n","    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    result = {key: round(value * 100, 4) for key, value in result.items()}\n","    return result\n","\n","#  Training Arguments\n","t_args = cfg['train']\n","cap_major = 0\n","if torch.cuda.is_available():\n","    cap_major = torch.cuda.get_device_capability()[0]\n","use_bf16 = (cap_major >= 8)\n","use_fp16 = (cap_major < 8) and torch.cuda.is_available()\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=CHECKPOINT_DIR,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"steps\",\n","    logging_steps=100,\n","    save_total_limit=2,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_rougeL\",\n","    greater_is_better=True,\n","    num_train_epochs=t_args['epochs'],\n","    per_device_train_batch_size=t_args['batch_size'],\n","    per_device_eval_batch_size=t_args['batch_size'],\n","    gradient_accumulation_steps=t_args['grad_accum'],\n","    learning_rate=t_args['lr'],\n","    warmup_ratio=t_args['warmup_ratio'],\n","    weight_decay=0.01,\n","    predict_with_generate=True,\n","    generation_max_length=cfg['tokenization']['max_target_len'],\n","    seed=SEED,\n","    data_seed=SEED,\n","    fp16=use_fp16,\n","    bf16=use_bf16,\n","    dataloader_num_workers=2,\n","    report_to=\"none\"\n",")\n","\n","# Initialize Trainer\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_ds,\n","    eval_dataset=eval_ds,\n","    processing_class=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","print(\"Trainer initialized. Ready for training.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vpnfqSYOi5VW","executionInfo":{"status":"ok","timestamp":1763180672871,"user_tz":480,"elapsed":6612,"user":{"displayName":"R Powers","userId":"06683789047320527470"}},"outputId":"5e3cdf05-46e1-4c0c-8ba3-d8680846b149"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Trainer initialized. Ready for training.\n"]}]},{"cell_type":"markdown","source":["## Training Configuration\n","\n","### Hyperparameter Choices\n","\n","| Hyperparameter | Value | Rationale |\n","|----------------|-------|-----------|\n","| Epochs | 3 | Standard for fine-tuning; validation loss plateaus by epoch 3 |\n","| Batch Size | 8 | Maximum that fits in Colab GPU memory |\n","| Gradient Accumulation | 2 | Effective batch size = 16, balancing stability and speed |\n","| Learning Rate | 5e-5 | Standard for fine-tuning transformers (Devlin et al., 2019) |\n","| Warmup Ratio | 0.1 | Gradual warmup prevents early training instability |\n","| Weight Decay | 0.01 | Mild regularization to prevent overfitting |\n","| Precision | FP16/BF16 | Mixed precision for faster training on GPU |\n","\n","### Evaluation Strategy\n","\n","- **Metric**: ROUGE-L (measures longest common subsequence)\n","- **Checkpoint Selection**: Save best model based on validation ROUGE-L\n","- **Generation**: `predict_with_generate=True` enables autoregressive decoding during evaluation\n","\n","### What I am looking for\n","\n","1. **Training Loss**: Should decrease steadily across epochs\n","2. **Validation Loss**: Should decrease then plateau (not increase = no overfitting)\n","3. **ROUGE-L**: Primary quality metric; higher is better"],"metadata":{"id":"h5jrdSm7d32G"}},{"cell_type":"code","source":["#  Run Training\n","\n","print(f\"Starting training on {len(train_ds)} examples...\")\n","print(f\"Checkpoints will be saved to {CHECKPOINT_DIR}\")\n","\n","\n","train_result = trainer.train(resume_from_checkpoint=False)\n","\n","# Save Artifacts (Guardrail #2)\n","trainer.save_model()\n","print(f\"Best model saved to: {CHECKPOINT_DIR}\")\n","\n","hist = trainer.state.log_history\n","metrics_path = os.path.join(RESULTS_DIR, \"baseline_bart_20k_metrics.json\")\n","\n","final_metrics = {\n","    \"model\": \"BART-base (Baseline B)\",\n","    \"note\": f\"Trained on {len(train_ds)} examples\",\n","    \"config_file\": CONFIG_PATH,\n","    \"final_eval_metrics\": hist[-1],\n","    \"log_history\": hist\n","}\n","with open(metrics_path, \"w\") as f:\n","    json.dump(final_metrics, f, indent=2)\n","\n","print(\"\\n--- Training Complete ---\")\n","print(f\"Final metrics saved to: {metrics_path}\")\n","print(\"\\nFinal Evaluation Metrics:\")\n","print(json.dumps(hist[-1], indent=2))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":910},"id":"dmM2lPdxi5MT","executionInfo":{"status":"ok","timestamp":1763182969882,"user_tz":480,"elapsed":2278830,"user":{"displayName":"R Powers","userId":"06683789047320527470"}},"outputId":"86e69571-2941-45d3-b923-cca0974a0f74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting training on 20000 examples...\n","Checkpoints will be saved to /content/drive/MyDrive/w266_project_final/models/bart_base_cnn_dm_20k\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n","  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n","  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3750/3750 37:56, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1</th>\n","      <th>Rouge2</th>\n","      <th>Rougel</th>\n","      <th>Rougelsum</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>2.084900</td>\n","      <td>1.778065</td>\n","      <td>42.660900</td>\n","      <td>19.959000</td>\n","      <td>29.014700</td>\n","      <td>39.654000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.885100</td>\n","      <td>1.729170</td>\n","      <td>42.443300</td>\n","      <td>19.605700</td>\n","      <td>28.967700</td>\n","      <td>39.486900</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.698500</td>\n","      <td>1.724090</td>\n","      <td>42.618100</td>\n","      <td>19.887200</td>\n","      <td>29.209100</td>\n","      <td>39.618100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n","  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n","  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n","  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n","  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n","  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n","  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n","  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n","  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n","  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","/usr/local/lib/python3.12/dist-packages/transformers/data/data_collator.py:740: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n","  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"]},{"output_type":"stream","name":"stdout","text":["Best model saved to: /content/drive/MyDrive/w266_project_final/models/bart_base_cnn_dm_20k\n","\n","--- Training Complete ---\n","Final metrics saved to: /content/drive/MyDrive/w266_project_final/results/baseline_bart_20k_metrics.json\n","\n","Final Evaluation Metrics:\n","{\n","  \"train_runtime\": 2277.3053,\n","  \"train_samples_per_second\": 26.347,\n","  \"train_steps_per_second\": 1.647,\n","  \"total_flos\": 3.639474999853056e+16,\n","  \"train_loss\": 1.945367948404948,\n","  \"epoch\": 3.0,\n","  \"step\": 3750\n","}\n"]}]},{"cell_type":"markdown","source":["## Training Results Analysis\n","\n","### Convergence Assessment\n","\n","| Epoch | Train Loss | Val Loss | ROUGE-L |\n","|-------|------------|----------|---------|\n","| 1 | 2.085 | 1.778 | 29.01 |\n","| 2 | 1.885 | 1.729 | 28.97 |\n","| 3 | 1.699 | 1.724 | 29.21 |\n","\n","**Observations:**\n","\n","1. **Training Loss**: Decreases consistently (2.08 → 1.70), indicating the model is learning.\n","\n","2. **Validation Loss**: Decreases then plateaus (1.78 → 1.73 → 1.72). The slight increase in epoch 3 (1.729 → 1.724) is negligible and does not indicate overfitting.\n","\n","3. **ROUGE-L**: Stable around 29, with best performance at epoch 3 (29.21). This is the checkpoint saved.\n","\n","### Comparison to Published Results\n","\n","| Model | ROUGE-L | Notes |\n","|-------|---------|-------|\n","| BART-large (Lewis et al., 2020) | ~40 | Full training set, large model |\n","| BART-base (Results) | 29.21 | 20K subset, base model |\n","| LEAD-3 (extractive baseline) | 24.91 | No training |\n","\n","Our BART-base achieves **+4.3 ROUGE-L over LEAD-3**, confirming the model has learned abstractive summarization. The gap to BART-large is expected given the smaller model and training subset.\n","\n","### Key Takeaway\n","\n","The baseline model is **successfully trained** and provides a valid foundation for reranking experiments. The model achieves competitive ROUGE scores and shows little to no signs of overfitting.\n","\n","---\n","\n","## Next Steps\n","\n","This trained model will be used in:\n","- **Notebook 03**: Generate K=5 candidate summaries per article\n","- **Notebook 04**: Score candidates with FactCC and NLI verifiers\n","- **Notebook 08**: Final evaluation on full test set"],"metadata":{"id":"pXpUYhaMc7Ww"}},{"cell_type":"code","source":[],"metadata":{"id":"opB6wX1I5knR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ragLA-VI5klZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CkCgKqUo5kjT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CQND7hhk5khI"},"execution_count":null,"outputs":[]}]}